{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "}\n",
    "categories = [\"finance\", \"industry\", \"employ\", \"autos\", \"stock\", \"estate\", \"consumer\", \"worldeconomy\", \"coin\", \"pension\", \"policy\", \"startup\"]\n",
    "start_date = \"20240731\"\n",
    "date_obj = dt.datetime.strptime(start_date, \"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_page_num(category: str, date: str):\n",
    "    pgn = 1\n",
    "    while True:\n",
    "        url = f\"https://news.daum.net/breakingnews/economic/{category}?page={pgn}&regDate={date}\"\n",
    "        res = requests.get(url, headers=hdr)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        if soup.select(\"a.btn_page.btn_next\") == []:\n",
    "            break\n",
    "        pgn += 10\n",
    "\n",
    "    last_page_num = int(soup.select(\"a.num_page\")[-1].get_text()) if soup.select(\"a.num_page\") else 1\n",
    "    return last_page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_monthly_day_list(date_obj) -> list:\n",
    "    current_month = date_obj.month\n",
    "    monthly_day_list = []\n",
    "\n",
    "    while date_obj.month == current_month:\n",
    "        monthly_day_list.append(date_obj.strftime(\"%Y%m%d\"))\n",
    "        date_obj -= dt.timedelta(days=1)\n",
    "\n",
    "    return monthly_day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_from_news(news_url: str):\n",
    "    try:\n",
    "        res = requests.get(news_url, headers=hdr)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        title = soup.select_one(\"h3.tit_view\").get_text() if soup.select(\"h3.tit_view\") else None\n",
    "        publisher = soup.select_one(\"h1.doc-title\").text.strip() if soup.select(\"h1.doc-title\") else None\n",
    "        reporter = soup.select('[dmcf-ptype=\"general\"]')[-1].get_text() if soup.select('[dmcf-ptype=\"general\"]') else None\n",
    "        content = \" \".join([i.get_text() for i in soup.select('[dmcf-ptype=\"general\"]')]) if soup.select('[dmcf-ptype=\"general\"]') else None\n",
    "\n",
    "        return title, publisher, reporter, content\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news data from {news_url}: {e}\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(category: str, date: str):\n",
    "    last_page_num = get_last_page_num(category, date)\n",
    "    news_links = []\n",
    "\n",
    "    for page_num in range(1, last_page_num + 1):\n",
    "        page_url = f\"https://news.daum.net/breakingnews/economic/{category}?page={page_num}&regDate={date}\"\n",
    "        res = requests.get(page_url, headers=hdr)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        news_links.extend([a['href'] for a in soup.select(\"ul.list_news2.list_allnews li strong.tit_thumb a.link_txt\")])\n",
    "\n",
    "    return news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = []\n",
    "\n",
    "num_months = 12  # 원하는 달만큼 생성\n",
    "\n",
    "for _ in range(num_months):\n",
    "    monthly_day_list = make_monthly_day_list(date_obj)\n",
    "    month_list.append(monthly_day_list)\n",
    "    \n",
    "    date_obj = dt.datetime.strptime(monthly_day_list[-1], \"%Y%m%d\")\n",
    "    date_obj = date_obj.replace(day=1) - dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    for category in categories:\n",
    "        for month in month_list:\n",
    "            data = {\n",
    "                \"title\": [],\n",
    "                \"category\": [],\n",
    "                \"article_url\": [],\n",
    "                \"date\": [],\n",
    "                \"publisher\": [],\n",
    "                \"reporter\": [],\n",
    "                \"content\": []\n",
    "            }\n",
    "\n",
    "            for date in month: # 여러 달 돌릴 때 수정 필요\n",
    "                news_links = get_news_links(category, date)\n",
    "                if not news_links:\n",
    "                    continue\n",
    "\n",
    "                with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                    results = list(executor.map(source_from_news, news_links))\n",
    "\n",
    "                for result, newslink in zip(results, news_links):\n",
    "                    if result is not None:\n",
    "                        title, publisher, reporter, content = result\n",
    "                        data[\"title\"].append(title)\n",
    "                        data[\"category\"].append(category)\n",
    "                        data[\"article_url\"].append(newslink)\n",
    "                        data[\"date\"].append(date)\n",
    "                        data[\"publisher\"].append(publisher)\n",
    "                        data[\"reporter\"].append(reporter)\n",
    "                        data[\"content\"].append(content)\n",
    "\n",
    "                # print(f\"{date} 완료\")\n",
    "            # month가 끝날 때 데이터를 저장\n",
    "            month_str = month[-1][:6]  # \"YYYYMM\" 형식으로 날짜 추출\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(f\"C:/Users/SesacPython/Desktop/dataset/뉴스추천시스템/news_data_{category}_{month_str}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"{month_str} 데이터 저장 완료\")\n",
    "        \n",
    "        print(f\"{category} 완료\")\n",
    "    print(\"모든 데이터 수집 및 저장 완료\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collect_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
